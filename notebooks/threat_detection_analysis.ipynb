{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cybersecurity Threat Detection - Exploratory Analysis\n",
    "\n",
    "This notebook demonstrates the complete workflow of the AI-driven cybersecurity threat detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_collection.collector import load_sample_threat_data\n",
    "from preprocessing.preprocessor import DataPreprocessor\n",
    "from feature_engineering.engineer import FeatureEngineer\n",
    "from model_training.trainer import RandomForestModel, GradientBoostingModel, ModelTrainer\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample threat data\n",
    "df = load_sample_threat_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "threat_counts = df['is_malicious'].value_counts()\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"Benign: {threat_counts[0]} ({threat_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Malicious: {threat_counts[1]} ({threat_counts[1]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='is_malicious')\n",
    "plt.title('Distribution of Threats')\n",
    "plt.xlabel('Is Malicious')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Benign', 'Threat'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features\n",
    "engineer = FeatureEngineer()\n",
    "df_engineered = engineer.engineer_features_pipeline(\n",
    "    df, \n",
    "    include_statistical=False,\n",
    "    include_temporal=False\n",
    ")\n",
    "\n",
    "print(f\"Original features: {df.shape[1]}\")\n",
    "print(f\"After engineering: {df_engineered.shape[1]}\")\n",
    "print(f\"\\nNew columns: {list(set(df_engineered.columns) - set(df.columns))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some engineered features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Packet size distribution\n",
    "axes[0, 0].hist([df_engineered[df_engineered['is_malicious']==0]['packet_size'],\n",
    "                 df_engineered[df_engineered['is_malicious']==1]['packet_size']],\n",
    "                label=['Benign', 'Threat'], alpha=0.7)\n",
    "axes[0, 0].set_title('Packet Size Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Port distribution\n",
    "axes[0, 1].scatter(df_engineered['source_port'], df_engineered['destination_port'],\n",
    "                   c=df_engineered['is_malicious'], alpha=0.5, cmap='coolwarm')\n",
    "axes[0, 1].set_title('Source vs Destination Port')\n",
    "axes[0, 1].set_xlabel('Source Port')\n",
    "axes[0, 1].set_ylabel('Destination Port')\n",
    "\n",
    "# Payload ratio\n",
    "if 'payload_ratio' in df_engineered.columns:\n",
    "    axes[1, 0].boxplot([df_engineered[df_engineered['is_malicious']==0]['payload_ratio'],\n",
    "                        df_engineered[df_engineered['is_malicious']==1]['payload_ratio']],\n",
    "                       labels=['Benign', 'Threat'])\n",
    "    axes[1, 0].set_title('Payload Ratio by Class')\n",
    "\n",
    "# Protocol distribution\n",
    "protocol_threat = df_engineered.groupby(['protocol', 'is_malicious']).size().unstack(fill_value=0)\n",
    "protocol_threat.plot(kind='bar', ax=axes[1, 1], alpha=0.7)\n",
    "axes[1, 1].set_title('Protocol Distribution by Threat Status')\n",
    "axes[1, 1].legend(['Benign', 'Threat'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor()\n",
    "data = preprocessor.preprocess_pipeline(\n",
    "    df_engineered,\n",
    "    target_col='is_malicious',\n",
    "    test_size=0.2,\n",
    "    scaling_method='standard'\n",
    ")\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Features: {data['feature_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Add models\n",
    "trainer.add_model(RandomForestModel(n_estimators=100, max_depth=20, random_state=42))\n",
    "trainer.add_model(GradientBoostingModel(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "\n",
    "print(\"Models added to trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "trainer.train_all(X_train, y_train)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = trainer.evaluate_all(X_test, y_test)\n",
    "\n",
    "# Get comparison DataFrame\n",
    "comparison_df = trainer.get_comparison_dataframe()\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot of metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_name in enumerate(comparison_df['Model']):\n",
    "    values = comparison_df.iloc[i][metrics].values\n",
    "    axes[0].bar(x + i*width, values, width, label=model_name)\n",
    "\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].set_xticks(x + width / 2)\n",
    "axes[0].set_xticklabels(metrics, rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[1].barh(comparison_df['Model'], comparison_df['F1-Score'])\n",
    "axes[1].set_xlabel('F1-Score')\n",
    "axes[1].set_title('F1-Score by Model')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest Model: {trainer.best_model_name}\")\n",
    "print(f\"Best F1-Score: {trainer.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for best model\n",
    "if trainer.best_model:\n",
    "    cm = np.array(trainer.best_model.metrics['confusion_matrix'])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Benign', 'Threat'],\n",
    "                yticklabels=['Benign', 'Threat'])\n",
    "    plt.title(f'Confusion Matrix - {trainer.best_model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(trainer.best_model.metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best model (if available)\n",
    "if trainer.best_model and hasattr(trainer.best_model.model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': data['feature_names'],\n",
    "        'importance': trainer.best_model.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(importance_df['feature'][:15], importance_df['importance'][:15])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 15 Features - {trainer.best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all trained models\n",
    "saved_paths = trainer.save_all_models('../models/trained')\n",
    "print(\"Models saved:\")\n",
    "for model_name, path in saved_paths.items():\n",
    "    print(f\"  {model_name}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Data loading and exploration\n",
    "2. Feature engineering for network traffic\n",
    "3. Data preprocessing and splitting\n",
    "4. Training multiple ML models\n",
    "5. Model evaluation and comparison\n",
    "6. Feature importance analysis\n",
    "\n",
    "The best performing model can now be used for real-time threat detection via the API service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
